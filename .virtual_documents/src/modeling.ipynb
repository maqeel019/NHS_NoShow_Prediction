# Imports
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import joblib

# Load dataset 
df = pd.read_csv("../data/processed/nhs_no_show_enhanced_synthetic.csv")
df



df.info()
df.describe()



# droping duplicate data
print("*"*30)
print("Duplicates before:", df.duplicated().sum())
df = df.drop_duplicates().reset_index(drop=True)
print("Duplicates after:", df.duplicated().sum())
print("*"*30)

print(df.isnull().sum())



print(df['NoShow'].unique())
print(df['Ethnicity'].unique())
print(df['Age_Group'].unique())
print(df['Gender'].unique())
print(df['Medical_Specialty'].unique())
print(df['Consultation_Type'].unique())
print(df['Appointment_Type'].unique())
print(df['Previous_Appointments'].unique())
print(df['Previous_NoShows'].unique())



# Dropping Base_NoShow_Prob and NoShow_Prob_Final because they are derived from other features. 
# Including them could give the model a hint and lead to overfitting.

df.drop(columns = ["Base_NoShow_Prob" , "NoShow_Prob_Final"] , inplace=True)
df


# Creating New feature
df["NoShowRate"] = df["Previous_NoShows"] / np.where(df["Previous_Appointments"] == 0, 1, df["Previous_Appointments"])

df["NoShowRate"] = df["NoShowRate"].round(3)
df.info()


import numpy as np
from sklearn.preprocessing import LabelEncoder

# NoShow currently "Yes"/"No" â€” convert to 1/0
df['NoShow'] = df['NoShow'].map({'Yes': 1, 'No': 0})
df['NoShow'] = df['NoShow'].astype(int)

# Simple label encoding
le_imd = LabelEncoder()
df['IMD_encoded'] = le_imd.fit_transform(df['IMD_Decile'].astype(str))

# Replacing each specialty with its relative frequency (0..1)
specialty_counts = df['Medical_Specialty'].value_counts(normalize=True)
df['Specialty_Freq'] = df['Medical_Specialty'].map(specialty_counts)

# D. One-hot encode other categorical columns
one_hot_cols = ['Ethnicity', 'Age_Group', 'Gender', 'Consultation_Type', 'Appointment_Type']

# Use get_dummies; drop_first=True to avoid perfect multicollinearity (optional)
df_encoded = pd.get_dummies(df, columns=one_hot_cols, drop_first=True)



# Final feature list

features = [
    'IMD_encoded',
    'Specialty_Freq',
    'Previous_Appointments',
    'Previous_NoShows',
    'NoShowRate'
]
# add all generated one-hot columns
one_hot_generated = [c for c in df_encoded.columns if any(c.startswith(col + '_') for col in one_hot_cols)]
features += one_hot_generated

X = df_encoded[features]
y = df_encoded['NoShow']


print("Encoded feature shape:", X.shape)
print("Sample feature columns:", X.columns[:20].tolist())
print("Target distribution:\n", y.value_counts(normalize=True))


from sklearn.model_selection import train_test_split

# Using 75% data for training and 25% for testing
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42, stratify=y
)

print("Training set shape:", X_train.shape)
print("Test set shape:", X_test.shape)
print("Training target distribution:\n", y_train.value_counts(normalize=True))
print("Test target distribution:\n", y_test.value_counts(normalize=True))



from sklearn.ensemble import RandomForestClassifier

# model
rf_model = RandomForestClassifier(
    n_estimators=200,         # number of trees
    max_depth=None,           # let trees grow fully
    random_state=42,
    class_weight='balanced',  # automatically adjust for class imbalance
    n_jobs=-1                 # use all CPU cores for speed
)

# training model
rf_model.fit(X_train, y_train)

print("Random Forest training completed!")



from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, roc_auc_score, RocCurveDisplay
import matplotlib.pyplot as plt

# Make predictions 
y_pred = rf_model.predict(X_test)
y_pred_prob = rf_model.predict_proba(X_test)[:, 1]  # probability for class 1 (NoShow)

# performance
acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_prob)
cm = confusion_matrix(y_test, y_pred)

print("Accuracy:", round(acc, 4))
print("F1 Score:", round(f1, 4))
print("ROC AUC:", round(roc_auc, 4))
print("\nConfusion Matrix:\n", cm)
print("\nClassification Report:\n", classification_report(y_test, y_pred))





from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report

# Model
lr = LogisticRegression(
    class_weight='balanced',  # handle imbalance
    max_iter=1000,            # ensure convergence
    random_state=42,
    n_jobs=-1
)

# Train
lr.fit(X_train, y_train)

# Predict
y_pred_lr = lr.predict(X_test)
y_prob_lr = lr.predict_proba(X_test)[:,1]

# Evaluate
acc = accuracy_score(y_test, y_pred_lr)
f1 = f1_score(y_test, y_pred_lr)
roc_auc = roc_auc_score(y_test, y_prob_lr)
cm = confusion_matrix(y_test, y_pred_lr)

print(" Logistic Regression ")
print("Accuracy:", round(acc, 4))
print("F1 Score:", round(f1, 4))
print("ROC AUC:", round(roc_auc, 4))
print("\nConfusion Matrix:\n", cm)
print("\nClassification Report:\n", classification_report(y_test, y_pred_lr))



import lightgbm as lgb

# Handle imbalance using scale_pos_weight
pos = y_train.sum()
neg = len(y_train) - pos
scale = neg / pos

lgb_model = lgb.LGBMClassifier(
    n_estimators=400,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.9,
    colsample_bytree=0.8,
    objective='binary',
    class_weight='balanced',  # alternative to scale_pos_weight
    random_state=42,
    n_jobs=-1
)

# Train
lgb_model.fit(X_train, y_train)

# Predict
y_pred_lgb = lgb_model.predict(X_test)
y_prob_lgb = lgb_model.predict_proba(X_test)[:,1]

# Evaluate
acc = accuracy_score(y_test, y_pred_lgb)
f1 = f1_score(y_test, y_pred_lgb)
roc_auc = roc_auc_score(y_test, y_prob_lgb)
cm = confusion_matrix(y_test, y_pred_lgb)

print(" LightGBM ")
print("Accuracy:", round(acc, 4))
print("F1 Score:", round(f1, 4))
print("ROC AUC:", round(roc_auc, 4))
print("\nConfusion Matrix:\n", cm)
print("\nClassification Report:\n", classification_report(y_test, y_pred_lgb))



from xgboost import XGBClassifier

# count ratio for weight
pos = y_train.sum()
neg = len(y_train) - pos
scale = neg / pos
print("scale_pos_weight =", scale)

xgb = XGBClassifier(
    n_estimators=400,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.9,
    colsample_bytree=0.8,
    objective='binary:logistic',
    scale_pos_weight=scale,
    random_state=42,
    n_jobs=-1,
    eval_metric='logloss'
)

xgb.fit(X_train, y_train)
print("XGBoost training completed!")



import numpy as np
from sklearn.metrics import f1_score

best_thr = 0
best_f1 = 0
y_prob = xgb.predict_proba(X_test)[:,1]

for thr in np.arange(0.05, 0.90, 0.01):
    y_pred_thr = (y_prob >= thr).astype(int)
    f1 = f1_score(y_test, y_pred_thr)
    if f1 > best_f1:
        best_f1 = f1
        best_thr = thr

print("Best threshold:", round(best_thr, 3))
print("Best F1 at that threshold:", round(best_f1, 4))



from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, accuracy_score

thr = 0.60

y_pred_opt = (xgb.predict_proba(X_test)[:,1] >= thr).astype(int)

print("Threshold used:", thr)
print("Accuracy:", round(accuracy_score(y_test, y_pred_opt), 4))
print("F1 Score:", round(f1_score(y_test, y_pred_opt), 4))
print("ROC AUC:", round(roc_auc_score(y_test, xgb.predict_proba(X_test)[:,1]), 4))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_opt))
print("\nClassification Report:\n", classification_report(y_test, y_pred_opt))



from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV

# Base model
xgb = XGBClassifier(
    objective='binary:logistic',
    scale_pos_weight = (len(y_train) - y_train.sum()) / y_train.sum(), 
    random_state=42,
    n_jobs=-1,
    eval_metric='logloss'
)


# Hyperparameter grid for random search
param_dist = {
    'n_estimators': [200, 400, 600],
    'max_depth': [3, 5, 6, 8],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'gamma': [0, 0.1, 0.3],
    'reg_alpha': [0, 0.01, 0.1],
    'reg_lambda': [1, 1.5, 2]
}

# RandomizedSearchCV
xgb_search = RandomizedSearchCV(
    estimator=xgb,
    param_distributions=param_dist,
    n_iter=25,          # number of random combinations to try
    scoring='f1',
    cv=3,
    verbose=1,
    random_state=42,
    n_jobs=-1
)



# Fit on training data
xgb_search.fit(X_train, y_train)

# Best model from search
xgb_best = xgb_search.best_estimator_

print("Best hyperparameters found:", xgb_search.best_params_)



import numpy as np
from sklearn.metrics import f1_score

y_prob = xgb_best.predict_proba(X_test)[:,1]

best_thr = 0
best_f1 = 0

for thr in np.arange(0.01, 0.90, 0.002):
    y_pred_thr = (y_prob >= thr).astype(int)
    f1 = f1_score(y_test, y_pred_thr)
    if f1 > best_f1:
        best_f1 = f1
        best_thr = thr

print("Best threshold:", round(best_thr, 3))
print("Best F1 at that threshold:", round(best_f1, 4))



from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, RocCurveDisplay
import matplotlib.pyplot as plt

# Predictions using optimal threshold
y_pred_opt = (y_prob >= best_thr).astype(int)

print("Threshold used:", best_thr)
print("Accuracy:", round(accuracy_score(y_test, y_pred_opt), 4))
print("F1 Score:", round(f1_score(y_test, y_pred_opt), 4))
print("ROC AUC:", round(roc_auc_score(y_test, y_prob), 4))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_opt))
print("\nClassification Report:\n", classification_report(y_test, y_pred_opt))



import joblib
joblib.dump(xgb_best, '../xgb_model.joblib')  # Better filename; saves in repo root


import xgboost
print('Notebook XGBoost version:', xgboost.__version__)
